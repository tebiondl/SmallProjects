{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXBzOdaLAEUn"
      },
      "source": [
        "##Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjLS1WetFhCE",
        "outputId": "0a4dc719-9cc7-4cd2-be7b-04716a4dd123"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from gymnasium.envs.toy_text.frozen_lake import generate_random_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBrRuhN1AQ-s"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4N2yVwtuFlBu"
      },
      "outputs": [],
      "source": [
        "GAMMA = 0.9\n",
        "MEMORY_SIZE = 1000\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 32\n",
        "EXPLORATION_MAX = 1\n",
        "EXPLORATION_MIN = 0.01\n",
        "EXPLORATION_DECAY = 0.995\n",
        "NUMBER_OF_EPISODES_FOR_TRAINING = 120\n",
        "NUMBER_OF_EPISODES_FOR_TESTING = 20\n",
        "ACTIVATION = \"relu\"\n",
        "OPTIMIZER = \"\"\n",
        "INITIALIZER = \"he_normal\"\n",
        "ENVIRONMENT = \"FrozenLake-v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cQV7IfhFOoSh"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "\n",
        "    def __init__(self,number_of_observations):\n",
        "        # Create replay memory\n",
        "        self.states = np.zeros((MEMORY_SIZE, number_of_observations))\n",
        "        self.states_next = np.zeros((MEMORY_SIZE, number_of_observations))\n",
        "        self.actions = np.zeros(MEMORY_SIZE, dtype=np.int32)\n",
        "        self.rewards = np.zeros(MEMORY_SIZE)\n",
        "        self.terminal_states = np.zeros(MEMORY_SIZE, dtype=bool)\n",
        "        self.current_size=0\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_next, terminal_state):\n",
        "        # Store a transition (s,a,r,s') in the replay memory\n",
        "        i = self.current_size\n",
        "        self.states[i] = state\n",
        "        self.states_next[i] = state_next\n",
        "        self.actions[i] = action\n",
        "        self.rewards[i] = reward\n",
        "        self.terminal_states[i] = terminal_state\n",
        "        self.current_size = i + 1\n",
        "\n",
        "    def sample_memory(self, batch_size):\n",
        "        # Generate a sample of transitions from the replay memory\n",
        "        batch = np.random.choice(self.current_size, batch_size)\n",
        "        states = self.states[batch]\n",
        "        states_next = self.states_next[batch]\n",
        "        rewards = self.rewards[batch]\n",
        "        actions = self.actions[batch]\n",
        "        terminal_states = self.terminal_states[batch]\n",
        "        return states, actions, rewards, states_next, terminal_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NZ6P4Gj0FtnU"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "\n",
        "    def __init__(self, number_of_observations, number_of_actions):\n",
        "        # Initialize variables and create neural model\n",
        "        self.exploration_rate = EXPLORATION_MAX\n",
        "        self.number_of_actions = number_of_actions\n",
        "        self.number_of_observations = number_of_observations\n",
        "        self.scores = []\n",
        "        self.memory = ReplayMemory(number_of_observations)\n",
        "        self.model = keras.models.Sequential()\n",
        "        self.model.add(keras.layers.Dense(24, input_shape=(number_of_observations,), \\\n",
        "                             activation=ACTIVATION,kernel_initializer=INITIALIZER))\n",
        "        self.model.add(keras.layers.Dense(24, activation=ACTIVATION,kernel_initializer=INITIALIZER))\n",
        "        self.model.add(keras.layers.Dense(number_of_actions, activation=\"linear\"))\n",
        "        self.model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, terminal_state):\n",
        "        # Store a tuple (s, a, r, s') for experience replay\n",
        "        state = np.reshape(state, [1, self.number_of_observations])\n",
        "        next_state = np.reshape(next_state, [1, self.number_of_observations])\n",
        "        self.memory.store_transition(state, action, reward, next_state, terminal_state)\n",
        "\n",
        "    def select(self, state):\n",
        "        # Generate an action for a given state using epsilon-greedy policy\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            return random.randrange(self.number_of_actions)\n",
        "        else:\n",
        "            state = np.reshape(state, [1, self.number_of_observations])\n",
        "            q_values = self.model.predict(state, verbose=0)\n",
        "            return np.argmax(q_values[0])\n",
        "\n",
        "    def select_greedy_policy(self, state):\n",
        "        # Generate an action for a given state using greedy policy\n",
        "        state = np.reshape(state, [1, self.number_of_observations])\n",
        "        q_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def learn(self):\n",
        "        # Learn the value Q using a sample of examples from the replay memory\n",
        "        if self.memory.current_size < BATCH_SIZE: return\n",
        "\n",
        "        states, actions, rewards, next_states, terminal_states = self.memory.sample_memory(BATCH_SIZE)\n",
        "\n",
        "        q_targets = self.model.predict(states, verbose=0)\n",
        "        q_next_states = self.model.predict(next_states, verbose=0)\n",
        "\n",
        "        for i in range(BATCH_SIZE):\n",
        "             if (terminal_states[i]):\n",
        "                  q_targets[i][actions[i]] = rewards[i]\n",
        "             else:\n",
        "                  q_targets[i][actions[i]] = rewards[i] + GAMMA * np.max(q_next_states[i])\n",
        "\n",
        "        self.model.train_on_batch(states, q_targets)\n",
        "\n",
        "        # Decrease exploration rate\n",
        "        self.exploration_rate *= EXPLORATION_DECAY\n",
        "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
        "\n",
        "    def add_score(self, score):\n",
        "       # Add the obtained score to a list to be presented later\n",
        "        self.scores.append(score)\n",
        "\n",
        "    def delete_scores(self):\n",
        "       # Delete the scores\n",
        "        self.scores = []\n",
        "\n",
        "    def display_scores_graphically(self):\n",
        "        # Display the obtained scores graphically\n",
        "        plt.plot(self.scores)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4LBloUSG0LmT"
      },
      "outputs": [],
      "source": [
        "def create_environment():\n",
        "    # Create simulated environment\n",
        "    environment = gym.make(ENVIRONMENT, desc=generate_random_map(size=8), map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")\n",
        "    number_of_observations = environment.observation_space.n\n",
        "    number_of_actions = environment.action_space.n\n",
        "    return environment, number_of_observations, number_of_actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbbw6blhDcsJ"
      },
      "source": [
        "## Training program\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yuzI0m5u5vVf",
        "outputId": "a7872c2c-0cfd-4979-ee43-14ce73a48b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0 0.0 False False {'prob': 0.3333333333333333}\n",
            "0\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 1 into shape (1,64)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m environment\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Store in memory the transition (s,a,r,s')\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremember\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminal_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Learn using a batch of experience stored in memory\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mDQN.remember\u001b[1;34m(self, state, action, reward, next_state, terminal_state)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremember\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, reward, next_state, terminal_state):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Store a tuple (s, a, r, s') for experience replay\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[1;32m---> 20\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumber_of_observations\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(next_state, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_observations])\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, terminal_state)\n",
            "File \u001b[1;32mc:\\Users\\mikib\\.conda\\envs\\my_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:285\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mikib\\.conda\\envs\\my_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
            "File \u001b[1;32mc:\\Users\\mikib\\.conda\\envs\\my_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(asarray(obj), method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
            "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (1,64)"
          ]
        }
      ],
      "source": [
        "environment, number_of_observations, number_of_actions = create_environment()\n",
        "agent = DQN(number_of_observations, number_of_actions)\n",
        "episode = 0\n",
        "start_time = time.perf_counter()\n",
        "while (episode < NUMBER_OF_EPISODES_FOR_TRAINING):\n",
        "    episode += 1\n",
        "    score = 0\n",
        "    state, info = environment.reset()\n",
        "    print(state)\n",
        "    end_episode = False\n",
        "    while not(end_episode):\n",
        "        # Select an action for the current state\n",
        "        action = agent.select(state)\n",
        "\n",
        "        # Execute the action on the environment\n",
        "        state_next, reward, terminal_state, truncated, info = environment.step(action)\n",
        "        \n",
        "        print(state_next, reward, terminal_state, truncated, info)\n",
        "        \n",
        "        #Pintar el entorno\n",
        "        environment.render()\n",
        "\n",
        "        # Store in memory the transition (s,a,r,s')\n",
        "        agent.remember(state, action, reward, state_next, terminal_state)\n",
        "\n",
        "        score += reward\n",
        "\n",
        "        # Learn using a batch of experience stored in memory\n",
        "        agent.learn()\n",
        "\n",
        "        # Detect end of episode\n",
        "        if terminal_state or truncated:\n",
        "            agent.add_score(score)\n",
        "            print(\"Episode {0:>3}: \".format(episode), end = '')\n",
        "            print(\"score {0:>3} \".format(math.trunc(score)), end = '')\n",
        "            print(\"(exploration rate: %.2f, \" % agent.exploration_rate, end = '')\n",
        "            print(\"transitions: \" + str(agent.memory.current_size) + \")\")\n",
        "            end_episode = True\n",
        "        else:\n",
        "            state = state_next\n",
        "\n",
        "print(\"Time for training:\", round((time.perf_counter() - start_time)/60), \"minutes\")\n",
        "print(\"Score (max):\", max(agent.scores))\n",
        "average_score = np.mean(agent.scores[max(0,(len(agent.scores)-10)):(len(agent.scores))])\n",
        "print(\"Score (average last 10 episodes):\", average_score)\n",
        "\n",
        "agent.display_scores_graphically()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSteJT6ULQVG"
      },
      "source": [
        "\n",
        "## Testing program\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIP0LQHGLZPj",
        "outputId": "906f931f-d01b-4147-f802-e3ea45f44045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode   1: score 500 \n",
            "Episode   2: score 500 \n",
            "Episode   3: score 500 \n",
            "Episode   4: score 500 \n",
            "Episode   5: score 500 \n",
            "Episode   6: score 500 \n",
            "Episode   7: score 500 \n",
            "Episode   8: score 263 \n",
            "Episode   9: score 500 \n",
            "Episode  10: score 500 \n",
            "Episode  11: score 500 \n",
            "Episode  12: score 304 \n",
            "Episode  13: score 492 \n",
            "Episode  14: score 279 \n",
            "Episode  15: score 500 \n",
            "Episode  16: score 500 \n",
            "Episode  17: score 500 \n",
            "Episode  18: score 500 \n",
            "Episode  19: score 500 \n",
            "Episode  20: score 500 \n",
            "Time for testing: 13 minutes\n",
            "Score (average): 466.9\n",
            "Score (max): 500.0\n"
          ]
        }
      ],
      "source": [
        "agent.delete_scores()\n",
        "episode = 0\n",
        "start_time = time.perf_counter()\n",
        "while (episode < NUMBER_OF_EPISODES_FOR_TESTING):\n",
        "    episode += 1\n",
        "    score = 0\n",
        "    state, info = environment.reset()\n",
        "    end_episode = False\n",
        "    while not(end_episode):\n",
        "        # Select an action for the current state\n",
        "        action = agent.select_greedy_policy(state)\n",
        "\n",
        "        # Execute the action in the environment\n",
        "        state_next, reward, terminal_state, truncated, info = environment.step(action)\n",
        "\n",
        "        score += reward\n",
        "\n",
        "        # Detect end of episode and print\n",
        "        if terminal_state or truncated:\n",
        "            agent.add_score(score)\n",
        "            print(\"Episode {0:>3}: \".format(episode), end = '')\n",
        "            print(\"score {0:>3} \\n\".format(math.trunc(score)), end = '')\n",
        "            end_episode = True\n",
        "        else:\n",
        "            state = state_next\n",
        "\n",
        "print(\"Time for testing:\", round((time.perf_counter() - start_time)/60), \"minutes\")\n",
        "print(\"Score (average):\", np.mean(agent.scores))\n",
        "print(\"Score (max):\", max(agent.scores))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
